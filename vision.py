# -*- coding: utf-8 -*-
"""PR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fa9lf9l3d6yKyi8--32pldxVBASZ_e7E
"""

#Import all the required libraries
!pip install wordcloud
!pip install gtts
!pip install playsound

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

import torch
import torchvision
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms
from PIL import Image
import string
import time
from sklearn.model_selection import train_test_split
from torchtext.data.utils import get_tokenizer

#used for creating Progress Meters or Pr
from tqdm import tqdm

from google.colab import drive
drive.mount('/content/drive')

"""Corrected error while importing playsound"""

# fixed error while importing playsound
!apt-get install libcairo2-dev

!pip install pycairo

import glob
from gtts import gTTS
from playsound import playsound
from IPython import display
import collections
import wordcloud
from wordcloud import WordCloud, STOPWORDS

import os
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Path to the folder containing images
images_dir = '/content/drive/MyDrive/fliker/Images'

# List files in the folder and its subdirectories
for dirname, _, filenames in os.walk(images_dir):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Count total number of images
all_imgs = [file for file in os.listdir(images_dir) if file.endswith('.jpg')]
print("The total images present in the dataset: {}".format(len(all_imgs)))

from PIL import Image
import matplotlib.pyplot as plt

# Visualising first 5 images:
Display_Images = all_imgs[:5]

figure, axes = plt.subplots(1, 5)
figure.set_figwidth(20)

for ax, image_filename in zip(axes, Display_Images):
    # Construct the full path to the image
    image_path = os.path.join(images_dir, image_filename)

    # Open and display the image
    img = Image.open(image_path)
    ax.imshow(img)
    ax.axis('off')

plt.show()

import random
from PIL import Image
from IPython.display import display
import os

random_image = random.choice(all_imgs)
random_image_path = os.path.join(images_dir, random_image)

# Open the image
random_img = Image.open(random_image_path)

# Display the image
display(random_img)

text_file = '/content/drive/MyDrive/fliker/captions.txt'
def load_doc(filename):
    open_file = open(text_file, 'r', encoding='latin-1' )
    text = open_file.read()
    open_file.close()
    return text
doc = load_doc(text_file)
print(doc[:300])

img_path = '/content/drive/MyDrive/fliker/Images/'

all_img_id = []
all_img_vector = []
annotations = []

with open('/content/drive/MyDrive/fliker/captions.txt' , 'r') as fo:
  next(fo)
  for line in fo :
    split_arr = line.split(',')
    all_img_id.append(split_arr[0])
    annotations.append(split_arr[1].rstrip('\n.')) #removing out the \n.
    all_img_vector.append(img_path+split_arr[0])

df = pd.DataFrame(list(zip(all_img_id, all_img_vector,annotations)),columns =['ID','Path', 'Captions'])

df

#check total captions and images present in dataset
print("Total captions present in the dataset: "+ str(len(annotations)))
print("Total images present in the dataset: " + str(len(all_imgs)))

#Create the vocabulary & the counter for the captions
vocabulary = [word.lower() for line in annotations for word in line.split()]
val_count = Counter(vocabulary)
val_count

#Visualise the top 30 occuring words in the captions
for word, count in val_count.most_common(30):
  print(word, ": ", count)

lst = val_count.most_common(30)
most_common_words_df = pd.DataFrame(lst, columns = ['Word', 'Count'])
most_common_words_df.plot.bar(x='Word', y='Count', width=0.6, color='orange', figsize=(15, 10))
plt.title("Top 30 maximum frequency words", fontsize = 18, color= 'navy')
plt.xlabel("Words", fontsize = 14, color= 'navy')
plt.ylabel("Count", fontsize = 14, color= 'navy')

wordcloud = WordCloud(width = 1000, height = 500).generate_from_frequencies(val_count)
plt.figure(figsize = (12, 12))
plt.imshow(wordcloud)

def caption_with_img_plot(image_id, frame) :
  capt = ("\n" *2).join(frame[frame['ID'] == image_id].Captions.to_list())
  fig, ax = plt.subplots()
  ax.set_axis_off()
  idx = df.ID.to_list().index(image_id)
  im =  Image.open(df.Path.iloc[idx])
  w, h = im.size[0], im.size[-1]
  ax.imshow(im)
  ax.text(w+50, h, capt, fontsize = 18, color = 'navy')
caption_with_img_plot(df.ID.iloc[99], df)

#data cleaning
rem_punct = str.maketrans('', '', string.punctuation)
for r in range(len(annotations)) :
  line = annotations[r]
  line = line.split()

  # converting to lowercase
  line = [word.lower() for word in line]

  # remove punctuation from each caption and hanging letters
  line = [word.translate(rem_punct) for word in line]
  line = [word for word in line if len(word) > 1]

  # remove numeric values
  line = [word for word in line if word.isalpha()]

  annotations[r] = ' '.join(line)

#add the <start> & <end> token to all those captions as well
annotations = ['<start>' + ' ' + line + ' ' + '<end>' for line in annotations]

#Create a list which contains all the path to the images
all_img_path = all_img_vector

##list contatining captions for an image
annotations[0:5]

import re
from collections import Counter
import torch

class CustomTokenizer:
    def __init__(self, top_word_cnt=5000):
        self.top_word_cnt = top_word_cnt
        self.word_to_index = {}
        self.index_to_word = {}
        self.oov_token = 'UNK'
        self.pad_token = 'PAD'

    def fit_on_texts(self, texts):
        # Tokenize and count words
        all_words = []
        for text in texts:
            words = re.findall(r'\b\w+\b', text.lower())
            all_words.extend(words)

        word_counts = Counter(all_words)
        most_common_words = word_counts.most_common(self.top_word_cnt - 1)  # -1 to make space for PAD token

        # Create word to index mapping
        self.word_to_index = {word: index + 1 for index, (word, _) in enumerate(most_common_words)}
        self.word_to_index[self.oov_token] = len(self.word_to_index)  # Index for OOV token
        self.word_to_index[self.pad_token] = 0  # Index for PAD token

        # Create index to word mapping
        self.index_to_word = {index: word for word, index in self.word_to_index.items()}

    def texts_to_sequences(self, texts):
        sequences = []
        for text in texts:
            words = re.findall(r'\b\w+\b', text.lower())
            sequence = [self.word_to_index.get(word, self.word_to_index[self.oov_token]) for word in words]
            sequences.append(sequence)
        return sequences

    def word_index(self, word):
        return self.word_to_index.get(word, self.word_to_index[self.oov_token])

    def index_word(self, index):
        return self.index_to_word.get(index, self.oov_token)

top_word_cnt = 5000
tokenizer = CustomTokenizer(top_word_cnt=top_word_cnt)

# Fit tokenizer on texts and transform each text into a sequence of integers
tokenizer.fit_on_texts(annotations)
train_seqs = tokenizer.texts_to_sequences(annotations)

# Equivalent code to print out-of-vocabulary token
print(tokenizer.word_to_index.get(tokenizer.oov_token, tokenizer.oov_token))

# Equivalent code to access word corresponding to index 0 (PAD token)
print(tokenizer.index_to_word.get(0, tokenizer.pad_token))

tokenizer.index_word

"""Corrected disabling grid."""

import re
from collections import Counter
import pandas as pd
import matplotlib.pyplot as plt

# Concatenate all texts in annotations and split them into words
tokenizer_top_words = [word for text in annotations for word in re.findall(r'\b\w+\b', text.lower())]

# Count occurrences of each word
tokenizer_top_words_count = Counter(tokenizer_top_words)

# Get the top 30 occurring words
tokens = tokenizer_top_words_count.most_common(30)

# Print top 30 words and their counts
for word, count in tokens:
    print(word, ": ", count)

# Create a DataFrame from the top 30 tokens
most_common_words_df = pd.DataFrame(tokens, columns=['Word', 'Count'])

# Plot the top 30 most common words
most_common_words_df.plot.bar(x='Word', y='Count', width=0.8, color='indigo', figsize=(17, 10))
plt.title('Top 30 common words', fontsize=20, color='navy')
plt.xlabel('Words', fontsize=14, color='navy')
plt.ylabel('Counts', fontsize=14, color='navy')

# Disable grid
plt.grid(False)

plt.show()

wordcloud_token = WordCloud(width = 1000, height = 500).generate_from_frequencies(tokenizer_top_words_count)
plt.figure(figsize = (12, 8))
plt.imshow(wordcloud_token)
plt.grid(False)

# Pad each vector to the max_length of the captions  store it to a vairable

import torch

# Calculate the maximum sequence length
max_length = max(len(seq) for seq in train_seqs)

# Pad sequences using PyTorch to the maximum length
cap_vector = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq, dtype=torch.int32) for seq in train_seqs],
                                             batch_first=True, padding_value=0)

print("The shape of Caption vector is:", cap_vector.shape)

# creating list to store preprocessed images and setting up the Image Shape

preprocessed_image = []
IMAGE_SHAPE = (299, 299)

"""Fixed the issue that only white grids were visible and images were not shown."""

import torch
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
from torchvision.utils import make_grid

# Define the base directory for image files
base_dir = '/content/drive/MyDrive/fliker/Images/'

# Define the preprocessing transformations
preprocess = transforms.Compose([
    transforms.Resize((299, 299)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

preprocessed_images = []

# Process each image
for img_filename in all_imgs[0:5]:
    # Construct the full path to the image
    img_path = os.path.join(base_dir, img_filename)

    # Read the image using PIL
    img = Image.open(img_path)

    # Apply preprocessing transformations
    img = preprocess(img)

    # Append preprocessed image to the list
    preprocessed_images.append(img)

# Stack the preprocessed images into a single tensor
preprocessed_images = torch.stack(preprocessed_images)

# Print the shape of the preprocessed images tensor
print("Shape of preprocessed images tensor:", preprocessed_images.shape)

# checking first five images post preprocessing
Display_Images = preprocessed_images[0:5]
figure, axes = plt.subplots(1,5)
figure.set_figwidth(25)
for ax, image in zip(axes, Display_Images):
    # Undo normalization to display the image
    image = image.permute(1, 2, 0)  # Reorder dimensions for displaying with matplotlib
    image = image * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])
    image = torch.clamp(image, 0, 1)  # Clip values to [0, 1] range
    ax.imshow(image)
    ax.axis('off')

plt.show()

from PIL import Image
import torchvision.transforms as transforms

# Define the preprocessing transformations
preprocess = transforms.Compose([
    transforms.Resize(IMAGE_SHAPE),  # Resize image to IMAGE_SHAPE
    transforms.ToTensor(),            # Convert image to PyTorch tensor
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize image
])

def load_images(image_path):
    # Read the image using PIL
    img = Image.open(image_path)

    # Apply preprocessing transformations
    img = preprocess(img)

    return img, image_path

all_img_vector

from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import os

class CustomDataset(Dataset):
    def __init__(self, image_paths, transform=None):
        self.image_paths = image_paths
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        return image, img_path

# Define preprocessing transformations
transform = transforms.Compose([
    transforms.Resize((299, 299)),  # Resize image to 299x299
    transforms.ToTensor(),           # Convert image to tensor
    # You can add more preprocessing transformations here if needed
])

# Create a list of image paths
training_list = sorted(set(all_img_vector))

# Create a custom dataset instance
custom_dataset = CustomDataset(training_list, transform=transform)

# Create a data loader
batch_size = 64
data_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=False, num_workers=0)

import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

# Define preprocessing transformations
preprocess = transforms.Compose([
    transforms.Resize(IMAGE_SHAPE),  # Resize image to IMAGE_SHAPE
    transforms.ToTensor(),            # Convert image to PyTorch tensor
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize image
])

# Create a custom dataset
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, img_paths, transform=None):
        self.img_paths = img_paths
        self.transform = transform

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):
        img_path = self.img_paths[idx]
        img = Image.open(img_path)
        if self.transform:
            img = self.transform(img)
        return img, img_path

# Create a dataset instance
new_img_dataset = CustomDataset(training_list, transform=preprocess)

# Create a data loader
new_img_loader = DataLoader(new_img_dataset, batch_size=64, shuffle=False)

# Optionally, you can iterate through the data loader
# for batch in new_img_loader:
#     images, paths = batch
#     # Process the batch as needed

#Ratio = 80:20 and we will set random state = 42
path_train, path_test, caption_train, caption_test = train_test_split(all_img_vector, cap_vector, test_size = 0.2, random_state = 42)

print("Training data for images: " + str(len(path_train)))
print("Testing data for images: " + str(len(path_test)))
print("Training data for Captions: " + str(len(caption_train)))
print("Testing data for Captions: " + str(len(caption_test)))

import torch
import torchvision.models as models

# Load the pre-trained InceptionV3 model
image_model = models.inception_v3(pretrained=True)

# Remove the classification layer (top) from the model
image_model = torch.nn.Sequential(*list(image_model.children())[:-2])

# Set the model to evaluation mode
image_model.eval()

# Print the model architecture to inspect the layers
print(image_model)

# Output the architecture and identify the appropriate layers to use

# Get the input and output of the model
# Assuming the first layer is a convolutional layer and the last layer before the final average pooling is a mixed layer

hidden_layer = image_model[-1]

# Define the feature extraction model
image_features_extract_model = torch.nn.Sequential( hidden_layer)

# Set the model to evaluation mode
image_features_extract_model.eval()

img_features = {}

# Iterate through the images in the dataset
for images, image_paths in tqdm(data_loader):
    # Forward pass through the feature extraction model
    with torch.no_grad():
        batch_features = image_features_extract_model(images)

    # Squeeze out the features in a batch
    batch_features_flattened = batch_features.view(batch_features.size(0), -1, batch_features.size(3))

    # Iterate through each image in the batch and store its features
    for batch_feat, path in zip(batch_features_flattened, image_paths):
        feature_path = path  # Already a string, no need to convert
        img_features[feature_path] = batch_feat.numpy()

batch_features

batch_features_flattened

batch_feat.shape

#view top five items of img_features dict
import more_itertools
top_5 = more_itertools.take(5, img_features.items())
top_5

#to provide, both images along with the captions as input
def map(image_name, caption):
    img_tensor = img_features[image_name.decode('utf-8')]
    return img_tensor, caption

from torch.utils.data import Dataset, DataLoader

class CustomDataset(Dataset):
    def __init__(self, img_paths, captions, transform=None):
        self.img_paths = img_paths
        self.captions = captions
        self.transform = transform

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):
        img_path = self.img_paths[idx]
        caption = self.captions[idx]

        # Load image
        img = Image.open(img_path)

        # Apply transformations if specified
        if self.transform:
            img = self.transform(img)

        return img, caption

def gen_dataset(img_paths, captions, transform=None, BATCH_SIZE = 64, shuffle=True):
    dataset = CustomDataset(img_paths, captions, transform)
    dataloader = DataLoader(dataset, BATCH_SIZE, shuffle=shuffle)
    return dataloader
BUFFER_SIZE = 1000
BATCH_SIZE = 64
train_dataset = gen_dataset(path_train,caption_train)
test_dataset = gen_dataset(path_test,caption_test)

sample_img_batch, sample_cap_batch = next(iter(train_dataset))
print(sample_img_batch.shape)  #(batch_size, 8*8, 2048)
print(sample_cap_batch.shape) #(batch_size,max_len)

# Setting  parameters

embedding_dim = 256
units = 512

#top 5,000 words +1
vocab_size = 5001
train_num_steps = len(path_train) // BATCH_SIZE
test_num_steps = len(path_test) // BATCH_SIZE

max_length = 31
feature_shape = batch_feat.shape[1]
attention_feature_shape = batch_feat.shape[0]

